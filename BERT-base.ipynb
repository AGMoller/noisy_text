{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4237,
     "status": "ok",
     "timestamp": 1598365957276,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "Z631aiQNgX7L",
    "outputId": "b9a94241-93ae-4809-c6f5-fe7a6e9bb62c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14007,
     "status": "ok",
     "timestamp": 1598365967057,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "06a3hIyqhV5W"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1598365986627,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "maiBzFGehh5Q"
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    \"\"\"\n",
    "    Convert UNINFORMATIVE to 0 and INFORMATIVE to 1\n",
    "    \"\"\"\n",
    "    if label == \"UNINFORMATIVE\": return 0\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1598365986955,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "O08703P5jSfm",
    "outputId": "c1f779ab-99f2-47a1-8d20-432eb2ff61af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fix seed for replicability\n",
    "seed=103\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1598365993806,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "rlwsJJj4jhUB"
   },
   "outputs": [],
   "source": [
    "def loadFile(file):\n",
    "    \"\"\"\n",
    "    Load file and apply preprocessing for BERT model\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    df.Label = df.Label.apply(lambda x: encode_label(x))\n",
    "\n",
    "    X = df.Text\n",
    "    y = df.Label\n",
    "\n",
    "    # Define tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Encode sentences to ids\n",
    "    input_ids = list()\n",
    "    for sent in tqdm(X):\n",
    "        encoded_sent = tokenizer.encode(sent, \n",
    "                                        add_special_tokens = True,\n",
    "                                        truncation = True,\n",
    "                                        max_length = 128) \n",
    "                                        #return_tensors = 'pt')\n",
    "\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # Pad/truncate sentences\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids,\n",
    "                                                                maxlen=128,\n",
    "                                                                dtype='long',\n",
    "                                                                value=0,\n",
    "                                                                truncating='post',\n",
    "                                                                padding='post')\n",
    "\n",
    "    # Attention Masks\n",
    "    attention_masks = list()\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    X = torch.tensor(input_ids)\n",
    "    y = torch.tensor(y)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return X, y, attention_masks\n",
    "\n",
    "def makeDataLoader(X, y, attention_masks):\n",
    "    \"\"\"\n",
    "    Make PyTorch iterator\n",
    "    \"\"\"\n",
    "    batch_size = 16\n",
    "\n",
    "    data = TensorDataset(X, attention_masks, y)\n",
    "    dataloader = DataLoader(data, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def f1(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "    '''Calculate F1 score. \n",
    "    Reference: https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "    '''\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "    \n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "        \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1224112,
     "status": "ok",
     "timestamp": 1598369036670,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "PsM8clvrknQu",
    "outputId": "4c5bd1e0-3978-4949-aee6-942b0d6ccb78"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    MAX_LEN = 64\n",
    "\n",
    "    train_data_path = \"data/train_lower_entities.tsv\"\n",
    "    test_data_path = \"data/valid_lower_entities.tsv\"\n",
    "\n",
    "    X_train, y_train, mask_train = loadFile(train_data_path)\n",
    "    X_test, y_test, mask_test = loadFile(test_data_path)\n",
    "\n",
    "    train = makeDataLoader(X_train, y_train, mask_train)\n",
    "    test = makeDataLoader(X_test, y_test, mask_test)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                        lr = 2e-5,\n",
    "                        eps = 1e-8)\n",
    "\n",
    "    epochs = 4\n",
    "\n",
    "    total_steps = len(train) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "    loss_values = list()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train), elapsed))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train)            \n",
    "        \n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy, eval_f1 = 0, 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test:\n",
    "            \n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            # Telling the model not to compute or store gradients, saving memory and\n",
    "            # speeding up validation\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)            # acc\n",
    "            tmp_eval_f1 = f1_score(np.argmax(logits, axis = 1).flatten(), label_ids.flatten(), average=\"weighted\")\n",
    "            \n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            eval_f1 += tmp_eval_f1\n",
    "\n",
    "            # Track the number of batches\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\"  F1: {0:.5f}\".format(eval_f1/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    model.save_pretrained(\"models/bert_text1\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMjuQZNB/74iOBJIyDxxwYS",
   "collapsed_sections": [],
   "name": "distilbert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
