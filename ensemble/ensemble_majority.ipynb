{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1597953108018,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "1vFpHWNT4mSS",
    "outputId": "4306b420-3e93-4a68-bcce-ac06427dc251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe483030d08>"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "os.chdir(\"/content/drive/My Drive/WNUT\")\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed=103\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1502,
     "status": "ok",
     "timestamp": 1597949804654,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "8JhdlkZK4udY"
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    \"\"\"\n",
    "    Convert UNINFORMATIVE to 0 and INFORMATIVE to 1\n",
    "    \"\"\"\n",
    "    if label == \"UNINFORMATIVE\": return 0\n",
    "    else: return 1\n",
    "\n",
    "def loadFile(file, device, tok:str):\n",
    "    \"\"\"\n",
    "    Load file and apply preprocessing for BERT model\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    df.Label = df.Label.apply(lambda x: encode_label(x))\n",
    "\n",
    "    X = df.Text\n",
    "    y = df.Label\n",
    "\n",
    "    # Define tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok)\n",
    "\n",
    "    # Encode sentences to ids\n",
    "    input_ids = list()\n",
    "    for sent in tqdm(X):\n",
    "        encoded_sent = tokenizer.encode(sent, \n",
    "                                        add_special_tokens = True,\n",
    "                                        truncation = True,\n",
    "                                        max_length = 128) \n",
    "                                        #return_tensors = 'pt')\n",
    "\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # Pad/truncate sentences\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids,\n",
    "                                                                maxlen=128,\n",
    "                                                                dtype='long',\n",
    "                                                                value=0,\n",
    "                                                                truncating='post',\n",
    "                                                                padding='post')\n",
    "\n",
    "    # Attention Masks\n",
    "    attention_masks = list()\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    X = torch.tensor(input_ids).to(device)\n",
    "    y = torch.tensor(y).to(device)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return X, y, attention_masks\n",
    "\n",
    "def makeDataLoader(X, y, attention_masks):\n",
    "    \"\"\"\n",
    "    Make PyTorch iterator\n",
    "    \"\"\"\n",
    "    batch_size = 16\n",
    "\n",
    "    data = TensorDataset(X, attention_masks, y)\n",
    "    dataloader = DataLoader(data, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def findDevice():\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "\n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def modelEval(model, data, device):\n",
    "\n",
    "    preds = np.array([]).reshape(0,2)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy, eval_f1 = 0, 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in data:\n",
    "        \n",
    "        # Add batch to cpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        preds = np.vstack((preds,logits))\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)   # acc\n",
    "        tmp_eval_f1 = f1_score(np.argmax(logits, axis = 1).flatten(), label_ids.flatten(), average=\"weighted\")                     # f1\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        eval_f1 += tmp_eval_f1\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  F1: {0:.5f}\".format(eval_f1/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131529,
     "status": "ok",
     "timestamp": 1597950699292,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "BkvcjbsE5lrj",
    "outputId": "d04b733b-950a-45fe-8993-d5dda1770a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1092.91it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1212.95it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1385.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.88294\n",
      "  F1: 0.88341\n",
      "  Validation took: 0:00:15\n",
      "  Accuracy: 0.89583\n",
      "  F1: 0.89588\n",
      "  Validation took: 0:00:15\n",
      "  Accuracy: 0.92063\n",
      "  F1: 0.92064\n",
      "  Validation took: 0:00:49\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    device = findDevice()\n",
    "\n",
    "    train_data_path = \"data/train_lower_entities.tsv\"\n",
    "    test_data_path = \"data/valid_lower_entities.tsv\"\n",
    "\n",
    "    X_base, y_base, mask_base = loadFile(test_data_path, device, \"bert-base-uncased\")\n",
    "    X_roberta, y_roberta, mask_roberta = loadFile(test_data_path, device, \"roberta-base\")\n",
    "    X_covid, y_covid, mask_covid = loadFile(test_data_path, device, \"digitalepidemiologylab/covid-twitter-bert\")\n",
    "\n",
    "    base = makeDataLoader(X_base, y_base, mask_base)\n",
    "    roberta = makeDataLoader(X_roberta, y_roberta, mask_roberta)\n",
    "    covid = makeDataLoader(X_covid, y_covid, mask_covid)\n",
    "\n",
    "    model_base = AutoModelForSequenceClassification.from_pretrained('models/bert_text').to(device)\n",
    "    model_roberta = AutoModelForSequenceClassification.from_pretrained('models/roberta_text').to(device)\n",
    "    model_covid = AutoModelForSequenceClassification.from_pretrained('models/covid-bert-fine-tuned1').to(device)\n",
    "\n",
    "    preds_base = modelEval(model_base, base, device)\n",
    "    preds_roberta = modelEval(model_roberta, roberta, device)\n",
    "    preds_covid = modelEval(model_covid, covid, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1597951046267,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "1h3uiy6KDryj"
   },
   "outputs": [],
   "source": [
    "preds_base_labels = np.argmax(preds_base, axis=1).flatten()\n",
    "preds_roberta_labels = np.argmax(preds_roberta, axis=1).flatten()\n",
    "preds_covid_labels = np.argmax(preds_covid, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1597952110216,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "0H7rP6bkKfF7"
   },
   "outputs": [],
   "source": [
    "feats = []\n",
    "for i in range(len(preds_base_labels)):\n",
    "    instance = [preds_base_labels[i], preds_roberta_labels[i], preds_covid_labels[i], \\\n",
    "                preds_base[i][0], preds_base[i][1],\\\n",
    "                preds_roberta[i][0], preds_roberta[i][1],\\\n",
    "                preds_covid[i][0], preds_covid[i][0]]\n",
    "    for j in range(len(instance)):\n",
    "        if type(instance[j]) == np.ndarray:\n",
    "            instance[j] = float(instance[j][0])\n",
    "        else:\n",
    "            instance[j] = float(instance[j])\n",
    "    feats.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1597953894254,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "JGGlOg-zVbfG"
   },
   "outputs": [],
   "source": [
    "# Hard Majority Voting\n",
    "\n",
    "finals = []\n",
    "for i in range(len(preds_base_labels)):\n",
    "  pred1 = preds_base_labels[i]\n",
    "  pred2 = preds_roberta_labels[i]\n",
    "  pred3 = preds_covid_labels[i]\n",
    "\n",
    "  zeros, ones = 0, 0\n",
    "\n",
    "  if pred1 == 0: zeros += 1\n",
    "  else: ones += 1\n",
    "\n",
    "  if pred2 == 0: zeros += 1\n",
    "  else: ones += 1\n",
    "\n",
    "  if pred3 == 0: zeros += 1\n",
    "  else: ones += 1\n",
    "\n",
    "  if zeros > ones: finals.append(0)\n",
    "  else: finals.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1597953910238,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "NjAS9kZudin5",
    "outputId": "837aa46f-303f-4a6f-ec5b-9d3cd42ea825",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999219297367631"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(finals, y_base.cpu(), average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1597954478450,
     "user": {
      "displayName": "Anders Møller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeXJESA3vcMASe_K8mvN0Uvek7plvNT5JDxBFRug=s64",
      "userId": "13568937180295096486"
     },
     "user_tz": -120
    },
    "id": "HFxSA1VPeaIH",
    "outputId": "03393fc9-6921-43e5-ebee-8c37a4dd8722"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8819092831059003"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soft Majority Voting\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "finals = []\n",
    "for i in range(len(preds_base_labels)):\n",
    "  base0, base1 = sigmoid(preds_base[i][0]),sigmoid(preds_base[i][1])\n",
    "  roberta0, roberta1 = sigmoid(preds_base[i][0]),sigmoid(preds_base[i][1])\n",
    "  covid0, covid1 = sigmoid(preds_base[i][0]),sigmoid(preds_base[i][1])\n",
    "\n",
    "  mean0, mean1 = np.mean(np.array((base0,roberta0,covid0))), np.mean(np.array((base1,roberta1,covid1)))\n",
    "\n",
    "  p = np.array((mean0,mean1))\n",
    "\n",
    "  finals.append(np.argmax(p).flatten()[0])\n",
    "\n",
    "#print(finals)\n",
    "\n",
    "f1_score(finals, y_base.cpu(), average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LARDC4P9fKZK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPyJ9h1tzAb+cjHjUX+n6O7",
   "collapsed_sections": [],
   "name": "transformer_ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
